{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cb70b-37b4-4085-9a20-d0cba8e290c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "By training models on different bootstraps, bagging reduces the variance of the individual models. It also avoids overfitting by exposing the constituent models to different parts of the dataset. The predictions from all the sampled models are then combined through a simple averaging to make the overall prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ece320-bd03-475f-a168-1e9fd392b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner. It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. One disadvantage of bagging is that it introduces a loss of interpretability of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2924f25-4b44-4021-ad6f-dd10a3a331f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "But if we try to decrease bias, the variance increases and vice-versa. Hence we need a optimal solution. This is the well known as bias-variance tradeoff. In ensemble learning weak learners also known as base models are used as building blocks for designing more complex models by combining several of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51f4d7-6fd6-4f0f-b8cf-4778148e096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "Classification and Regression: Bagging is widely used for classification and regression tasks. Classification helps improve the accuracy of predictions by combining the outputs of multiple classifiers trained on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb095d5a-5602-41cb-95c8-755c8ddd22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "The role of ensemble size in bagging (Bootstrap Aggregating) is critical for determining the performance and stability of the model. Here are the key points regarding ensemble size in bagging:\n",
    "\n",
    "Improvement of Performance: Increasing the number of models in the ensemble generally improves performance by reducing variance and enhancing generalization. This is because bagging leverages multiple models to average out the errors.\n",
    "\n",
    "Reduction of Variance: A larger ensemble helps in reducing the model variance. Each model in the ensemble is trained on a different bootstrap sample of the data, which means the predictions of individual models will vary. By averaging the predictions, the overall variance of the ensemble decreases.\n",
    "\n",
    "Diminishing Returns: After a certain point, adding more models to the ensemble yields diminishing returns. The initial models added to the ensemble contribute significantly to performance improvement, but the benefit of each additional model reduces after a certain ensemble size.\n",
    "\n",
    "Computational Cost: Larger ensembles require more computational resources for training and prediction. It's essential to balance the benefits of adding more models with the available computational resources and time constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
